# ETL Pipeline с использованием Apache Airflow

Проект реализует ETL pipeline для расчёта витрины активности клиентов. Процесс включает извлечение данных, их трансформацию и сохранение результатов. Система построена на основе Apache Airflow.

---

## Структура проекта

```
.
├── data/                      # Данные для обработки
│   ├── profit_table.csv       # Входной файл с данными о транзакциях
│   └── flags_activity.csv     # Выходной файл с результатами
├── scripts/                   # Скрипты
│   └── transform_script.py    # Скрипт для трансформации данных
├── dag.py                     # DAG файл для Airflow
├── Dockerfile                 # Конфигурация Docker для сборки контейнера
├── requirements.txt           # Зависимости Python
└── README.md                  # Документация (этот файл)
```

---

## Предварительные требования

Перед началом работы убедитесь, что у вас установлены следующие инструменты:
1. **Docker** (версия 20.10+)
2. **Docker Compose** (опционально, если вы планируете использовать `docker-compose`)

---

## Как запустить проект

### 1. Сборка Docker-образа

Чтобы собрать Docker-образ, выполните команду:

```bash
docker build --no-cache --progress=plain -t my-airflow:latest .
```

### 2. Запуск Docker-контейнера

Для запуска контейнера выполните следующую команду:

```bash
docker run -d -p 8080:8080 my-airflow:latest standalone
```

- Контейнер будет работать в режиме "всё в одном" (standalone), включая веб-сервер Airflow, базу данных и планировщик.

### 3. Доступ к веб-интерфейсу Airflow

Откройте браузер и перейдите по адресу:

```
http://localhost:8080
```

Войдите с использованием следующих данных:
- **Логин:** `airflow`
- **Пароль:** необходимо найти в логах контейнера, в строке, содержащей текст: "Login with username: admin  password:"

---

## Проверка файлов и данных в контейнере

Для проверки содержимого папок в контейнере выполните следующие команды:

1. **Проверка DAG:**
   ```bash
   docker exec -it <CONTAINER_ID> ls /opt/airflow/dags
   ```

2. **Проверка данных:**
   ```bash
   docker exec -it <CONTAINER_ID> ls /opt/airflow/data
   ```

---

## Описание процесса ETL

### 1. **Extract (Извлечение)**
Данные загружаются из файла `profit_table.csv`.

### 2. **Transform (Трансформация)**
Трансформация данных выполняется с использованием предоставленного скрипта `transform_script.py`.

### 3. **Load (Загрузка)**
Результаты сохраняются в файл `flags_activity.csv`. Новые данные добавляются к существующим без перезаписи.

---

## Примечания

1. **Если вы обновили файлы или код:**
   При внесении изменений в файлы или скрипты пересоберите образ:
   ```bash
   docker build --no-cache --progress=plain -t my-airflow:latest .
   ```

2. **Если вам нужно перезапустить контейнер:**
   ```bash
   docker stop <CONTAINER_ID>
   docker rm <CONTAINER_ID>
   docker run -d -p 8080:8080 my-airflow:latest standalone
   ```

3. **Логи выполнения DAG:**
   Вы можете проверить логи выполнения задач через веб-интерфейс Airflow.
---

### Пример команды для очистки кеша Docker:

Если сборка образа вызывает проблемы из-за кеша, выполните команду:

```bash
docker builder prune
```

После этого повторите сборку Docker-образа.
